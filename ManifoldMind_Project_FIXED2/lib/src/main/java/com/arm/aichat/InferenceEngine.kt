package com.arm.aichat

import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.StateFlow

/**
 * High-level inference interface for the local llama.cpp engine.
 *
 * The implementation is backed by JNI (see :lib/src/main/cpp).
 */
interface InferenceEngine {

    sealed class State {
        data object Uninitialized : State()
        data object Initializing : State()
        data object ModelNotLoaded : State()
        data object LoadingModel : State()
        data object ModelReady : State()
        data object Generating : State()
        data class Error(val message: String) : State()
    }

    /** Current engine lifecycle state. */
    val state: StateFlow<State>

    /** Stream of text chunks generated by the model (incremental). */
    val tokenFlow: Flow<String>

    /** Loads a GGUF model from disk and prepares the runtime. */
    suspend fun loadModel(modelPath: String)

    /** Sends (or resets) a system prompt (chat template is handled natively). */
    suspend fun sendSystemPrompt(systemPrompt: String)

    /** Sends a user prompt and begins generation (tokens emitted on [tokenFlow]). */
    suspend fun sendUserPrompt(userPrompt: String)

    /** Unloads model and resets native state. */
    suspend fun unloadModel()

    /** Frees all native resources. After calling, the instance should not be reused. */
    fun destroy()
}

class UnsupportedArchitectureException : Exception()
